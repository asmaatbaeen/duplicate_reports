{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bc054da-df32-49bc-8317-cfdb9b33430e",
   "metadata": {},
   "source": [
    "## Identifying duplicate\n",
    "\n",
    "This notebook focuses on the evaluation of various techniques for identifying duplicate. It aims to explore and compare different methods to determine which approach is most effective in identifying and handling duplicates within a dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92c1a4b4-2bce-4836-a6d7-7be678773767",
   "metadata": {},
   "source": [
    "## Import Twitter data \n",
    "The data is stored in cpt.pARQUET. hge sample dataset includes x entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7225522-45bc-469a-8a48-55f27ca43f89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# need to specify this to show full text in one line\n",
    "pd.options.display.max_colwidth = 0\n",
    "\n",
    "df_path = \"../data/data.csv\"\n",
    "df = pd.read_csv(df_path)\n",
    "\n",
    "# just printing the informaition \n",
    "print(\"dataframe shape\", df.shape)\n",
    "print(\"--------------------------------\")\n",
    "print(\"the columns feed value counts\", df.feed.value_counts())\n",
    "print(\"-------------------------------\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab166896",
   "metadata": {},
   "source": [
    "### Modify Report to Include Duplicated Rows in the DataFrame\n",
    "\n",
    "The objective of this modification is to include the duplicated rows in the DataFrame, along with an additional column indicating whether each row is duplicated or not. By doing so, we can have a clear view of the duplicated data and facilitate further analysis.\n",
    "\n",
    "To achieve this, the following steps were taken:\n",
    "\n",
    "**The full dataset:**\n",
    "\n",
    "- The dataset was split into two samples: one containing duplicate rows and the other containing non-duplicate rows. This division ensured that both types of data were included for analysis.\n",
    "\n",
    "**Duplicate Sample with ChatGPT:**\n",
    "\n",
    "- The duplicate sample was subjected to the chatGPT process, which involved restructuring the text to generate variations while preserving the essence of the original information. This step was crucial in simulating different text structures for the duplicated rows.\n",
    "\n",
    "**Labeling Duplicate Rows:**\n",
    "\n",
    "- In order to differentiate between duplicate and non-duplicate rows, a new column was added to the DataFrame. This additional column indicates whether each row is a duplicate or not. By labeling the rows accordingly, we can easily identify and analyze the duplicated data within the dataset.\n",
    "\n",
    "By incorporating these modifications, the DataFrame now includes the duplicated rows, allowing for a comprehensive assessment of the data. The new column serves as a valuable indicator for distinguishing between duplicate and non-duplicate entries, enabling efficient analysis and further processing of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4218752",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['report_description','feed']\n",
    "sample = 300\n",
    "\n",
    "sample_filter = df.loc[:,cols]\n",
    "\n",
    "# Select the sample that is going to be passed to chatGPT to generate duplicates\n",
    "duplicate_set = sample_filter.sample(n=sample, replace=False)\n",
    "\n",
    "# Select the second sample, excluding the data that is passed to chatGPT for duplicate generation\n",
    "non_duplicate_set = sample_filter.drop(duplicate_set.index).sample(n=sample, replace=False)\n",
    "\n",
    "# Add label column\n",
    "duplicate_set['state'] = ['duplicated'] * sample\n",
    "duplicate_set = duplicate_set.reset_index(drop=True)\n",
    "\n",
    "# Add the repeated word column to the DataFrame\n",
    "non_duplicate_set['state'] = ['not duplicated'] * sample\n",
    "non_duplicate_set = non_duplicate_set.reset_index(drop=True)\n",
    "\n",
    "non_duplicate_set.head(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18f8a31e",
   "metadata": {},
   "source": [
    "### chatGPT\n",
    "\n",
    "You are doing some prompt engineering with chatGPT. My suggestion:\n",
    "Move this piece of code out - put it in a separate file and the call the method and comment it out\n",
    "Write a comment around if you have AN API KEY THEN YOU CAN UNCOMMENT THIS CODE OUT (ll the code for chatgpt together)\n",
    "\n",
    "You should:\n",
    "Run this notebook and save the chatgpt data in the parquet file. Once-off so do as separate script\n",
    "If the key is dicontinued - then use chatgpt manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3fc320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values, find_dotenv\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from scripts import rewrite_text\n",
    "\n",
    "config = dotenv_values(find_dotenv())\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "# Set up OpenAI API credentials\n",
    "from dotenv import dotenv_values, find_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "config = dotenv_values(find_dotenv())\n",
    "  \n",
    "openai.api_key = \"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\"\n",
    "  \n",
    "duplicate_set_gpt = duplicate_set.copy()\n",
    "\n",
    "duplicate_set_gpt[\"report_description\"] = duplicate_set[\"report_description\"].apply(rewrite_text)\n",
    "duplicate_set_gpt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d305c01",
   "metadata": {},
   "source": [
    "### show the actual and Paraphrased sentences side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f90422",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_set[\"openai_report_description\"] = duplicate_set_gpt[\"report_description\"]\n",
    "duplicate_set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c4750c3",
   "metadata": {},
   "source": [
    "#### put all togther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "### make a non-duplicated report for each report in the sample \n",
    "non_duplicate_reports[\"openai_report_description\"] = duplicate_reports[\"report_description\"]\n",
    "\n",
    "# combine both samples\n",
    "sample_filter = pd.concat([non_duplicate_reports,duplicate_reports])\n",
    "sample_filter = sample_filter.sample(frac=1, random_state=42)\n",
    "sample_filter = sample_filter.reset_index(drop=True)\n",
    "sample_filter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cfb908d-44d6-41d3-aba8-b2d97214e402",
   "metadata": {},
   "source": [
    "## Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    " \n",
    " \n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def perform_entity_recognition(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    return  ', '.join(entities)\n",
    "\n",
    " \n",
    "\n",
    "# Apply entity recognition to the 'text' column\n",
    "sample_filter['entities'] = sample_filter['report_description'].apply(perform_entity_recognition)\n",
    "sample_filter['entities_openai'] = sample_filter['openai_report_description'].apply(perform_entity_recognition)\n",
    "sample_filter = sample_filter.reset_index(drop=True)\n",
    "\n",
    "sample_filter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9be87219",
   "metadata": {},
   "source": [
    "## cosine similarity using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b5eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "threshold = 70\n",
    "def add_label_column(df, existing_column, threshold):\n",
    "    df['predicted state'] = df[existing_column].apply(lambda x: 'not duplicated' if x < threshold else 'duplicated')\n",
    "    return df\n",
    "\n",
    "def is_duplicate_cosine_similarity(df,  cols,threshold):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix1 = vectorizer.fit_transform(df[cols[0]])\n",
    "    tfidf_matrix2 = vectorizer.transform(df[cols[1]])\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix1, tfidf_matrix2)\n",
    "    # print(cosine_sim.diagonal())\n",
    "\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'report_description': df[cols[0]],\n",
    "        'openai_report_description': df[cols[1]],\n",
    "        'Similarity': cosine_sim.diagonal()*100,\n",
    "        'state': df[cols[2]]\n",
    "    })\n",
    "    \n",
    "    \n",
    "    comparison_df =  add_label_column(comparison_df, 'Similarity', threshold)\n",
    " \n",
    "    return comparison_df\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "is_duplicate_cosine_similarity_df = is_duplicate_cosine_similarity(sample_filter, ['report_description',\"openai_report_description\",\"state\"],threshold)\n",
    "is_duplicate_cosine_similarity_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b0ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_duplicate_cosine_similarity_df = (is_duplicate_cosine_similarity_df['state'] == is_duplicate_cosine_similarity_df['predicted state']).mean()\n",
    "is_duplicate_cosine_similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dbcc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_is_duplicate_cosine_similarity = is_duplicate_cosine_similarity(sample_filter,['report_description','entities',\"state\"],threshold)\n",
    " \n",
    "accuracy_entities_is_duplicate_cosine_similarity = (entities_is_duplicate_cosine_similarity['state'] == entities_is_duplicate_cosine_similarity['predicted state']).mean()\n",
    "accuracy_entities_is_duplicate_cosine_similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "889798b3",
   "metadata": {},
   "source": [
    "## fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d733894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "\n",
    "def is_duplicate_fuzzywuzzy(df, cols, threshold=70):\n",
    "    comparison_results = []\n",
    "     \n",
    "    for index, row in df.iterrows():\n",
    "        text1 = row[cols[0]]\n",
    "        text2 = row[cols[1]]\n",
    "        # print( row)\n",
    "        similarity_ratio = fuzz.token_set_ratio(text1, text2)\n",
    "        label =  row[cols[2]]\n",
    "        if similarity_ratio >= threshold:\n",
    "            comparison_results.append({\n",
    "                'Text1': text1,\n",
    "                'Text2': text2,\n",
    "                'Similarity ratio': similarity_ratio,\n",
    "                \"predicted state\":\"duplicated\",\n",
    "                'state': label\n",
    "            })\n",
    "        else:\n",
    "\n",
    "            \n",
    "             \n",
    "            comparison_results.append({\n",
    "                'Text1': text1,\n",
    "                'Text2': text2,\n",
    "                'Similarity ratio': int(similarity_ratio),\n",
    "                \"predicted state\":\"not duplicated\",\n",
    "                \"state\":label\n",
    "            })\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_results)\n",
    "    return comparison_df\n",
    " \n",
    "\n",
    "is_duplicate_fuzzywuzzy_df = is_duplicate_fuzzywuzzy(sample_filter, ['report_description',\"openai_report_description\",\"state\"],threshold)\n",
    "is_duplicate_fuzzywuzzy_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942bc4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_is_duplicate_fuzzywuzzy_df = (is_duplicate_fuzzywuzzy_df['state'] == is_duplicate_fuzzywuzzy_df['predicted state']).mean()\n",
    "accuracy_is_duplicate_fuzzywuzzy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f2e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_is_duplicate_fuzzywuzzy = is_duplicate_fuzzywuzzy(sample_filter,['report_description','entities',\"state\"],threshold)\n",
    "\n",
    "accuracy_entities_is_duplicate_fuzzywuzzy = (entities_is_duplicate_fuzzywuzzy['state'] == entities_is_duplicate_fuzzywuzzy['predicted state']).mean()\n",
    "accuracy_entities_is_duplicate_fuzzywuzzy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1dccc28",
   "metadata": {},
   "source": [
    "## Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d4d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein\n",
    " \n",
    "\n",
    "# Function to compare texts using Levenshtein distance and return similarity as a percentage\n",
    "def is_duplicate_Levenshtein(df,cols,threshold):\n",
    "    similarities = []\n",
    "    for text1, text2 in zip(df[cols[0]], df[cols[1]]):\n",
    "        distance = Levenshtein.distance(text1, text2)\n",
    "        max_length = max(len(text1), len(text2))\n",
    "        similarity = (max_length - distance) / max_length * 100\n",
    "        similarities.append(int(similarity))\n",
    "    \n",
    "    comparison_df = pd.DataFrame({\n",
    "        'report_description': df[cols[0]],\n",
    "        'openai_report_description': df[cols[1]],\n",
    "        'Similarity Percentage':  similarities,\n",
    "         \"state\": df[cols[2]]        \n",
    "    })\n",
    "    comparison_df =  add_label_column(comparison_df, 'Similarity Percentage', threshold)\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Compare texts using Levenshtein distance and return similarity as a percentage\n",
    "is_duplicate_Levenshtein_df = is_duplicate_Levenshtein(sample_filter, ['report_description',\"openai_report_description\",\"state\"],threshold)\n",
    "\n",
    "is_duplicate_Levenshtein_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa22fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_is_duplicate_Levenshtein_df_df = (is_duplicate_Levenshtein_df['state'] == is_duplicate_Levenshtein_df['predicted state']).mean()\n",
    "accuracy_is_duplicate_Levenshtein_df_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff44d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_is_duplicate_Levenshtein = is_duplicate_Levenshtein(sample_filter,['report_description','entities',\"state\"],threshold)\n",
    "\n",
    "accuracy_entities_is_duplicate_Levenshtein = (entities_is_duplicate_Levenshtein['state'] == entities_is_duplicate_Levenshtein['predicted state']).mean()\n",
    "accuracy_entities_is_duplicate_Levenshtein\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96cf100a",
   "metadata": {},
   "source": [
    "## embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3d1a755",
   "metadata": {},
   "source": [
    " ##### SentenceTransformer('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148eed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    " \n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "# Function to compare texts using SentenceTransformer\n",
    "def compare_texts_sentence_transformer1(df, cols,threshold):\n",
    "    embeddings1 = model.encode(df[cols[0]].tolist())\n",
    "    embeddings2 = model.encode(df[cols[1]].tolist())\n",
    "    similarities = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Text1': df[cols[0]],\n",
    "        'Text2': df[cols[1]],\n",
    "        'Similarity Percentage': similarities.diagonal()*100,\n",
    "         \"state\": df[cols[2]] \n",
    "          \n",
    "    })\n",
    "    comparison_df =  add_label_column(comparison_df, 'Similarity Percentage', threshold)\n",
    "    comparison_df['Similarity Percentage'] = comparison_df['Similarity Percentage'].astype(int)\n",
    "    return comparison_df\n",
    "\n",
    "# Compare texts using SentenceTransformer\n",
    "is_duplicate_SentenceTransformer1_df = compare_texts_sentence_transformer1(sample_filter, ['report_description',\"openai_report_description\",\"state\"],threshold)\n",
    "\n",
    "is_duplicate_SentenceTransformer1_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bea422",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_is_duplicate_SentenceTransformer1_df = (is_duplicate_SentenceTransformer1_df['state'] == is_duplicate_SentenceTransformer1_df['predicted state']).mean()\n",
    "accuracy_is_duplicate_SentenceTransformer1_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec162033",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_is_duplicate_SentenceTransformer1 = compare_texts_sentence_transformer1(sample_filter,['report_description','entities',\"state\"],threshold)\n",
    " \n",
    "accuracy_entities_is_duplicate_SentenceTransformer1 = (entities_is_duplicate_SentenceTransformer1['state'] == entities_is_duplicate_SentenceTransformer1['predicted state']).mean()\n",
    "accuracy_entities_is_duplicate_SentenceTransformer1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7e1fe7b",
   "metadata": {},
   "source": [
    "##### SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510432e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    " \n",
    "\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to compare texts using SentenceTransformer\n",
    "def compare_texts_sentence_transformer2(df, cols,threshold):\n",
    "    # embeddings1 = model.encode(df[cols[0]].tolist())\n",
    "    # embeddings2 = model.encode(df[cols[1]].tolist())\n",
    "    # similarities = cosine_similarity(embeddings1, embeddings2)\n",
    "    embeddings1 = model.encode(df[cols[0]].tolist())\n",
    "    embeddings2 = model.encode(df[cols[1]].tolist())\n",
    "    similarities = util.cos_sim(embeddings1, embeddings2)\n",
    "   \n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Text1': df[cols[0]],\n",
    "        'Text2': df[cols[1]],\n",
    "        'Similarity Percentage':similarities.diagonal()*100,\n",
    "         \"state\": df[cols[2]] \n",
    "          \n",
    "    })\n",
    "    comparison_df =  add_label_column(comparison_df, 'Similarity Percentage', threshold)\n",
    "    comparison_df['Similarity Percentage'] = comparison_df['Similarity Percentage'].astype(int)\n",
    "    return comparison_df\n",
    "\n",
    "# Compare texts using SentenceTransformer\n",
    "is_duplicate_SentenceTransformer2_df = compare_texts_sentence_transformer2(sample_filter, ['report_description',\"openai_report_description\",\"state\"],threshold)\n",
    "\n",
    "is_duplicate_SentenceTransformer2_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9ec4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_is_duplicate_SentenceTransformer2_df = (is_duplicate_SentenceTransformer2_df['state'] == is_duplicate_SentenceTransformer2_df['predicted state']).mean()\n",
    "accuracy_is_duplicate_SentenceTransformer2_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0879a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_is_duplicate_SentenceTransformer2_df = compare_texts_sentence_transformer2(sample_filter,['report_description','entities',\"state\"],threshold)\n",
    "accuracy_entities_is_duplicate_SentenceTransformer2_df = (entities_is_duplicate_SentenceTransformer2_df['state'] == entities_is_duplicate_SentenceTransformer2_df['predicted state']).mean()\n",
    "accuracy_entities_is_duplicate_SentenceTransformer2_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3de38f1c",
   "metadata": {},
   "source": [
    "\n",
    "at the moment is just comparing the text in whole report with the entities - it make sinse to mark the pair as duplicated if they contain the same entities and in that case the prediction would be a bit higher I think "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4483b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Method':[\"SentenceTransformer1\",\"SentenceTransformer2\",\"Levenshtein\",\"fuzzywuzzy\",\"cosine_similarity\"],\n",
    "    'text prediction':  [accuracy_is_duplicate_SentenceTransformer1_df,accuracy_is_duplicate_SentenceTransformer2_df,accuracy_is_duplicate_Levenshtein_df_df,accuracy_is_duplicate_fuzzywuzzy_df,is_duplicate_cosine_similarity_df],  \n",
    "    'entities text prediction': [accuracy_entities_is_duplicate_SentenceTransformer2_df,accuracy_entities_is_duplicate_SentenceTransformer1,accuracy_entities_is_duplicate_Levenshtein,accuracy_entities_is_duplicate_fuzzywuzzy,accuracy_entities_is_duplicate_cosine_similarity]\n",
    "})\n",
    "df['text prediction'] = df['text prediction']*100\n",
    "df['entities text prediction'] = df['entities text prediction']*100\n",
    "# Display the DataFrame\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845d9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shannon-0s_KSgZI-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
